{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7265b16a",
   "metadata": {},
   "source": [
    "# Name: Waldiez Flow\n",
    "\n",
    "## Description: A waldiez flow\n",
    "\n",
    "## Tags: \n",
    "\n",
    "###ðŸ§© generated with â¤ï¸ by Waldiez.\n",
    "\n",
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8173d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# # !{sys.executable} -m pip install -q ag2[openai]==0.10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4798d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671b458",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "import sys\n",
    "import threading\n",
    "import traceback\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from types import ModuleType\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Coroutine,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import autogen  # type: ignore\n",
    "from autogen import (\n",
    "    Agent,\n",
    "    AssistantAgent,\n",
    "    Cache,\n",
    "    ChatResult,\n",
    "    ConversableAgent,\n",
    "    GroupChat,\n",
    "    UserProxyAgent,\n",
    "    register_function,\n",
    "    runtime_logging,\n",
    ")\n",
    "from autogen.agentchat import GroupChatManager, ReplyResult, run_group_chat\n",
    "from autogen.agentchat.group import (\n",
    "    AgentTarget,\n",
    "    ContextExpression,\n",
    "    ContextVariables,\n",
    "    ExpressionAvailableCondition,\n",
    "    ExpressionContextCondition,\n",
    "    NestedChatTarget,\n",
    "    OnContextCondition,\n",
    "    ReplyResult,\n",
    "    RevertToUserTarget,\n",
    ")\n",
    "from autogen.agentchat.group.patterns import DefaultPattern\n",
    "from autogen.agentchat.group.patterns.pattern import Pattern\n",
    "from autogen.events import BaseEvent\n",
    "from autogen.io.run_response import (\n",
    "    AsyncRunResponseProtocol,\n",
    "    RunResponseProtocol,\n",
    ")\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Common environment variable setup for Waldiez flows\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"AUTOGEN_USE_DOCKER\"] = \"0\"\n",
    "os.environ[\"TOGETHER_NO_BANNER\"] = \"1\"\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n",
    "#\n",
    "# let's try to avoid:\n",
    "# module 'numpy' has no attribute '_no_nep50_warning'\"\n",
    "# ref: https://github.com/numpy/numpy/blob/v2.2.2/doc/source/release/2.2.0-notes.rst#nep-50-promotion-state-option-removed\n",
    "os.environ[\"NEP50_DEPRECATION_WARNING\"] = \"0\"\n",
    "os.environ[\"NEP50_DISABLE_WARNING\"] = \"1\"\n",
    "os.environ[\"NPY_PROMOTION_STATE\"] = \"weak\"\n",
    "if not hasattr(np, \"_no_pep50_warning\"):\n",
    "\n",
    "    import contextlib\n",
    "    from typing import Generator\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def _np_no_nep50_warning() -> Generator[None, None, None]:\n",
    "        \"\"\"Dummy function to avoid the warning.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        None\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "        yield\n",
    "\n",
    "    setattr(np, \"_no_pep50_warning\", _np_no_nep50_warning)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1573b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logging() -> None:\n",
    "    \"\"\"Start logging.\"\"\"\n",
    "    runtime_logging.start(\n",
    "        logger_type=\"sqlite\",\n",
    "        config={\"dbname\": \"flow.db\"},\n",
    "    )\n",
    "\n",
    "\n",
    "start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de20e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Load model API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdde26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# This section assumes that a file named:\n",
    "# \"waldiez_flow_api_keys.py\"\n",
    "# exists in the same directory as this file.\n",
    "# This file contains the API keys for the models used in this flow.\n",
    "# It should be .gitignored and not shared publicly.\n",
    "# If this file is not present, you can either create it manually\n",
    "# or change the way API keys are loaded in the flow.\n",
    "\n",
    "\n",
    "def load_api_key_module(flow_name: str) -> ModuleType:\n",
    "    \"\"\"Load the api key module.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flow_name : str\n",
    "        The flow name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ModuleType\n",
    "        The api keys loading module.\n",
    "    \"\"\"\n",
    "    module_name = f\"{flow_name}_api_keys\"\n",
    "    if module_name in sys.modules:\n",
    "        return importlib.reload(sys.modules[module_name])\n",
    "    return importlib.import_module(module_name)\n",
    "\n",
    "\n",
    "__MODELS_MODULE__ = load_api_key_module(\"waldiez_flow\")\n",
    "\n",
    "\n",
    "def get_waldiez_flow_model_api_key(model_name: str) -> str:\n",
    "    \"\"\"Get the model api key.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        The model name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The model api key.\n",
    "    \"\"\"\n",
    "    return __MODELS_MODULE__.get_waldiez_flow_model_api_key(model_name)\n",
    "\n",
    "\n",
    "class GroupDict(TypedDict):\n",
    "    \"\"\"Group related global dict.\"\"\"\n",
    "\n",
    "    chats: dict[str, GroupChat]\n",
    "    patterns: dict[str, Pattern]\n",
    "\n",
    "\n",
    "__GROUP__: GroupDict = {\"chats\": {}, \"patterns\": {}}\n",
    "\n",
    "__AGENTS__: dict[str, ConversableAgent] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41c9dc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e913e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"Replace this with your code.\n",
    "\n",
    "Add any code here that will be placed at the top of the whole flow.\n",
    "\"\"\"\n",
    "\n",
    "# Example:\n",
    "# global variable\n",
    "# DATABASE = {\n",
    "#     \"users\": [\n",
    "#         {\"id\": 1, \"name\": \"Alice\"},\n",
    "#         {\"id\": 2, \"name\": \"Bob\"},\n",
    "#     ],\n",
    "#     \"posts\": [\n",
    "#         {\"id\": 1, \"title\": \"Hello, world!\", \"author_id\": 1},\n",
    "#         {\"id\": 2, \"title\": \"Another post\", \"author_id\": 2},\n",
    "#     ],\n",
    "# }\n",
    "#\n",
    "# Add your code below\n",
    "\n",
    "redundant_agent_names = [\"agent_a\", \"agent_b\", \"agent_c\"]\n",
    "\n",
    "\n",
    "def initiate_task(\n",
    "    task: Annotated[str, \"The task to be processed by multiple agents\"],\n",
    "    task_type: Annotated[\n",
    "        str, \"Type of task: 'creative', 'problem_solving', 'factual', etc.\"\n",
    "    ],\n",
    "    context_variables: ContextVariables,\n",
    ") -> ReplyResult:\n",
    "    \"\"\"\n",
    "    Initiate processing of a task across multiple redundant agents with different approaches\n",
    "    \"\"\"\n",
    "    context_variables[\"task_initiated\"] = True\n",
    "    context_variables[\"task_completed\"] = False\n",
    "    context_variables[\"evaluation_complete\"] = False\n",
    "    context_variables[\"current_task\"] = task\n",
    "    context_variables[\"task_type\"] = task_type\n",
    "\n",
    "    # Reset previous results\n",
    "    context_variables[\"agent_a_result\"] = None\n",
    "    context_variables[\"agent_b_result\"] = None\n",
    "    context_variables[\"agent_c_result\"] = None\n",
    "    context_variables[\"evaluation_scores\"] = {}\n",
    "    context_variables[\"final_result\"] = None\n",
    "    context_variables[\"selected_approach\"] = None\n",
    "\n",
    "    return ReplyResult(\n",
    "        message=f\"Task initiated: '{task}' (Type: {task_type}). Will process with multiple independent approaches.\",\n",
    "        context_variables=context_variables,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_and_select(\n",
    "    evaluation_notes: Annotated[\n",
    "        str, \"Detailed evaluation of each agent's result\"\n",
    "    ],\n",
    "    score_a: Annotated[int, \"Score for Agent A's approach (1-10 scale)\"],\n",
    "    score_b: Annotated[int, \"Score for Agent B's approach (1-10 scale)\"],\n",
    "    score_c: Annotated[int, \"Score for Agent C's approach (1-10 scale)\"],\n",
    "    selected_result: Annotated[str, \"The selected or synthesized final result\"],\n",
    "    selection_rationale: Annotated[\n",
    "        str,\n",
    "        \"Explanation for why this result was selected or how it was synthesized\",\n",
    "    ],\n",
    "    context_variables: ContextVariables,\n",
    ") -> ReplyResult:\n",
    "    \"\"\"\n",
    "    Evaluate the different approaches and select or synthesize the best result\n",
    "    \"\"\"\n",
    "    # Create scores dictionary from individual parameters\n",
    "    scores = {\"agent_a\": score_a, \"agent_b\": score_b, \"agent_c\": score_c}\n",
    "\n",
    "    context_variables[\"evaluation_notes\"] = evaluation_notes\n",
    "    context_variables[\"evaluation_scores\"] = scores\n",
    "    context_variables[\"final_result\"] = selected_result\n",
    "    context_variables[\"evaluation_complete\"] = True\n",
    "\n",
    "    # Determine which approach was selected (highest score)\n",
    "    max_score = 0\n",
    "    selected_approach = None\n",
    "    for agent, score in scores.items():\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            selected_approach = agent\n",
    "    context_variables[\"selected_approach\"] = selected_approach\n",
    "\n",
    "    return ReplyResult(\n",
    "        message=f\"Evaluation complete. Selected result: {selection_rationale[:100]}...\",\n",
    "        context_variables=context_variables,\n",
    "        target=RevertToUserTarget(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab28d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860278e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_1_mini_llm_config: dict[str, Any] = {\n",
    "    \"model\": \"gpt-4.1-mini\",\n",
    "    \"api_type\": \"openai\",\n",
    "    \"api_key\": get_waldiez_flow_model_api_key(\"gpt_4_1_mini\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86172de1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "User = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    description=\"A new User agent\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,\n",
    "    llm_config=False,\n",
    ")\n",
    "\n",
    "__AGENTS__[\"User\"] = User\n",
    "\n",
    "agent_a = AssistantAgent(\n",
    "    name=\"agent_a\",\n",
    "    description=\"A new Assistant agent\",\n",
    "    system_message=\"You are Agent A, specializing in a structured, analytical approach to tasks.\\n\\n        For creative tasks:\\n        - Use structured frameworks and established patterns\\n        - Follow proven methodologies and best practices\\n        - Focus on clarity, organization, and logical progression\\n\\n        For problem-solving tasks:\\n        - Use first principles thinking and systematic analysis\\n        - Break down problems into component parts\\n        - Consider established solutions and scientific approaches\\n\\n        For factual information:\\n        - Prioritize objective, verifiable data\\n        - Present information in a structured, hierarchical manner\\n        - Focus on accuracy and comprehensiveness\\n\\n        Always identify your approach clearly and explain your methodology as part of your response.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_mini_llm_config,\n",
    "        ],\n",
    "        cache_seed=None,\n",
    "    ),\n",
    ")\n",
    "\n",
    "__AGENTS__[\"agent_a\"] = agent_a\n",
    "\n",
    "agent_b = AssistantAgent(\n",
    "    name=\"agent_b\",\n",
    "    description=\"A new Assistant agent\",\n",
    "    system_message=\"You are Agent B, specializing in a creative, lateral-thinking approach to tasks.\\n\\n        For creative tasks:\\n        - Use metaphors, analogies, and unexpected connections\\n        - Think outside conventional frameworks\\n        - Explore unique perspectives and novel combinations\\n\\n        For problem-solving tasks:\\n        - Use creative ideation and divergent thinking\\n        - Look for non-obvious connections and innovative approaches\\n        - Consider unconventional solutions outside the mainstream\\n\\n        For factual information:\\n        - Present information through narratives and examples\\n        - Use contextual understanding and practical applications\\n        - Focus on making information relatable and engaging\\n\\n        Always identify your approach clearly and explain your methodology as part of your response.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_mini_llm_config,\n",
    "        ],\n",
    "        cache_seed=None,\n",
    "    ),\n",
    ")\n",
    "\n",
    "__AGENTS__[\"agent_b\"] = agent_b\n",
    "\n",
    "agent_c = AssistantAgent(\n",
    "    name=\"agent_c\",\n",
    "    description=\"A new Assistant agent\",\n",
    "    system_message=\"You are Agent C, specializing in a thorough, comprehensive approach to tasks.\\n\\n        For creative tasks:\\n        - Combine multiple perspectives and diverse inputs\\n        - Draw from cross-disciplinary knowledge and varied examples\\n        - Focus on thoroughness and covering all possible angles\\n\\n        For problem-solving tasks:\\n        - Consider multiple solution pathways simultaneously\\n        - Evaluate trade-offs and present alternative approaches\\n        - Focus on robustness and addressing edge cases\\n\\n        For factual information:\\n        - Present multiple perspectives and nuanced views\\n        - Include historical context and future implications\\n        - Focus on depth and breadth of coverage\\n\\n        Always identify your approach clearly and explain your methodology as part of your response.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_mini_llm_config,\n",
    "        ],\n",
    "        cache_seed=None,\n",
    "    ),\n",
    ")\n",
    "\n",
    "__AGENTS__[\"agent_c\"] = agent_c\n",
    "\n",
    "evaluator_agent = ConversableAgent(\n",
    "    name=\"evaluator_agent\",\n",
    "    description=\"A new Assistant agent\",\n",
    "    system_message=\"You are the Evaluator Agent responsible for assessing multiple approaches to the same task and selecting or synthesizing the best result.\\n\\n        Your role is to:\\n        1. Carefully review each approach and result\\n        2. Evaluate each solution based on criteria appropriate to the task type\\n        3. Assign scores to each approach on a scale of 1-10\\n        4. Either select the best approach or synthesize a superior solution by combining strengths\\n\\n        For creative tasks, evaluate based on:\\n        - Originality and uniqueness\\n        - Effectiveness in addressing the creative brief\\n        - Quality of execution and coherence\\n\\n        For problem-solving tasks, evaluate based on:\\n        - Correctness and accuracy\\n        - Efficiency and elegance\\n        - Comprehensiveness and robustness\\n\\n        For factual tasks, evaluate based on:\\n        - Accuracy and correctness\\n        - Comprehensiveness and depth\\n        - Clarity and organization\\n\\n        When appropriate, rather than just selecting a single approach, synthesize a superior solution by combining the strengths of multiple approaches.\\n\\n        Use the evaluate_and_select tool to submit your final evaluation, including detailed scoring and rationale.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,\n",
    "    functions=[\n",
    "        evaluate_and_select,\n",
    "    ],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_mini_llm_config,\n",
    "        ],\n",
    "        cache_seed=None,\n",
    "    ),\n",
    ")\n",
    "\n",
    "__AGENTS__[\"evaluator_agent\"] = evaluator_agent\n",
    "\n",
    "taskmaster_agent = ConversableAgent(\n",
    "    name=\"taskmaster_agent\",\n",
    "    description=\"A new Assistant agent\",\n",
    "    system_message=\"You are the Task Manager responsible for initiating tasks and coordinating the redundant pattern workflow.\\n\\n        Your role is to:\\n        1. Understand the user's request and frame it as a clear task\\n        2. Determine the appropriate task type (creative, problem_solving, factual)\\n        3. Initiate the task to be processed by multiple independent agents\\n        4. Return to the user with the final selected or synthesized result\\n\\n        For each request:\\n        1. Use the initiate_task tool to start the process\\n        2. After all agents have submitted their results and evaluation is complete, present the final result to the user\\n\\n        Always explain to the user that their task is being processed by multiple approaches to ensure the best possible outcome.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,\n",
    "    functions=[\n",
    "        initiate_task,\n",
    "    ],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_mini_llm_config,\n",
    "        ],\n",
    "        cache_seed=None,\n",
    "    ),\n",
    ")\n",
    "\n",
    "__AGENTS__[\"taskmaster_agent\"] = taskmaster_agent\n",
    "\n",
    "\n",
    "def nested_chat_message_taskmaster_agen_To_agent_a(\n",
    "    recipient: ConversableAgent,\n",
    "    messages: list[dict[str, Any]],\n",
    "    sender: ConversableAgent,\n",
    "    config: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Extracts the task to give to an agent as the task\"\"\"\n",
    "    return sender.context_variables.get(\n",
    "        \"current_task\", \"There's no task, return UNKNOWN.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def nested_chat_message_taskmaster_agen_To_agent_b(\n",
    "    recipient: ConversableAgent,\n",
    "    messages: list[dict[str, Any]],\n",
    "    sender: ConversableAgent,\n",
    "    config: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Extracts the task to give to an agent as the task\"\"\"\n",
    "    return sender.context_variables.get(\n",
    "        \"current_task\", \"There's no task, return UNKNOWN.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def nested_chat_message_taskmaster_agen_To_agent_c(\n",
    "    recipient: ConversableAgent,\n",
    "    messages: list[dict[str, Any]],\n",
    "    sender: ConversableAgent,\n",
    "    config: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Extracts the task to give to an agent as the task\"\"\"\n",
    "    return sender.context_variables.get(\n",
    "        \"current_task\", \"There's no task, return UNKNOWN.\"\n",
    "    )\n",
    "\n",
    "\n",
    "taskmaster_agent_handoff_nested_chat_queue: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": True,\n",
    "        \"chat_id\": 0,\n",
    "        \"recipient\": agent_a,\n",
    "        \"message\": nested_chat_message_taskmaster_agen_To_agent_a,\n",
    "    },\n",
    "    {\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": True,\n",
    "        \"chat_id\": 1,\n",
    "        \"recipient\": agent_b,\n",
    "        \"message\": nested_chat_message_taskmaster_agen_To_agent_b,\n",
    "    },\n",
    "    {\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": True,\n",
    "        \"chat_id\": 2,\n",
    "        \"recipient\": agent_c,\n",
    "        \"message\": nested_chat_message_taskmaster_agen_To_agent_c,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "taskmaster_agent.handoffs.add_context_condition(\n",
    "    condition=OnContextCondition(\n",
    "        target=NestedChatTarget(\n",
    "            nested_chat_config={\n",
    "                \"chat_queue\": taskmaster_agent_handoff_nested_chat_queue\n",
    "            }\n",
    "        ),\n",
    "        condition=ExpressionContextCondition(\n",
    "            expression=ContextExpression(\n",
    "                \"len(${agent_a_result}) == 0 or len(${agent_b_result}) == 0 or len(${agent_c_result}) == 0\"\n",
    "            )\n",
    "        ),\n",
    "        available=ExpressionAvailableCondition(\n",
    "            expression=ContextExpression(\n",
    "                \"${task_initiated} == True and len(${current_task}) > 0 and ${task_completed} == False\"\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "taskmaster_agent.handoffs.add_context_condition(\n",
    "    condition=OnContextCondition(\n",
    "        target=AgentTarget(evaluator_agent),\n",
    "        condition=ExpressionContextCondition(\n",
    "            expression=ContextExpression(\"${evaluation_complete} == False\")\n",
    "        ),\n",
    "        available=ExpressionAvailableCondition(\n",
    "            expression=ContextExpression(\"${task_completed} == True\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "taskmaster_agent.handoffs.set_after_work(target=RevertToUserTarget())\n",
    "\n",
    "evaluator_agent.handoffs.set_after_work(target=RevertToUserTarget())\n",
    "\n",
    "__INITIAL_MSG__ = \"I need help with this task: Write a short story about a robot learning to understand emotions.\"\n",
    "\n",
    "_pattern = DefaultPattern(\n",
    "    initial_agent=taskmaster_agent,\n",
    "    agents=[taskmaster_agent, evaluator_agent],\n",
    "    user_agent=User,\n",
    "    group_manager_args={\n",
    "        \"llm_config\": autogen.LLMConfig(\n",
    "            config_list=[\n",
    "                gpt_4_1_mini_llm_config,\n",
    "            ],\n",
    "            cache_seed=None,\n",
    "        ),\n",
    "        \"name\": \"\",\n",
    "    },\n",
    "    context_variables=ContextVariables(\n",
    "        data={\n",
    "            \"task_initiated\": False,\n",
    "            \"task_completed\": False,\n",
    "            \"evaluation_complete\": False,\n",
    "            \"current_task\": \"\",\n",
    "            \"task_type\": None,\n",
    "            \"approach_count\": 0,\n",
    "            \"agent_a_result\": None,\n",
    "            \"agent_b_result\": None,\n",
    "            \"agent_c_result\": None,\n",
    "            \"evaluation_scores\": {},\n",
    "            \"final_result\": None,\n",
    "            \"selected_approach\": None,\n",
    "            \"has_error\": False,\n",
    "            \"error_message\": \"\",\n",
    "            \"error_source\": \"\",\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n",
    "__GROUP__[\"patterns\"][\"_pattern\"] = _pattern\n",
    "\n",
    "\n",
    "def get_sqlite_out(dbname: str, table: str, csv_file: str) -> None:\n",
    "    \"\"\"Convert a sqlite table to csv and json files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dbname : str\n",
    "        The sqlite database name.\n",
    "    table : str\n",
    "        The table name.\n",
    "    csv_file : str\n",
    "        The csv file name.\n",
    "    \"\"\"\n",
    "    # pylint: disable=broad-exception-caught,too-many-try-statements\n",
    "    try:\n",
    "        conn = sqlite3.connect(dbname)\n",
    "    except BaseException:\n",
    "        return\n",
    "    query = f\"SELECT * FROM {table}\"  # nosec\n",
    "    try:\n",
    "        cursor = conn.execute(query)\n",
    "    except BaseException:\n",
    "        conn.close()\n",
    "        return\n",
    "    try:\n",
    "        rows = cursor.fetchall()\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        data = [dict(zip(column_names, row, strict=True)) for row in rows]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except BaseException:\n",
    "        try:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "        except BaseException:\n",
    "            pass\n",
    "        return\n",
    "    try:\n",
    "        with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            csv_writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "            csv_writer.writeheader()\n",
    "            csv_writer.writerows(data)\n",
    "        json_file = csv_file.replace(\".csv\", \".json\")\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\", newline=\"\\n\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "    except BaseException:\n",
    "        return\n",
    "\n",
    "\n",
    "def stop_logging() -> None:\n",
    "    \"\"\"Stop logging.\"\"\"\n",
    "    runtime_logging.stop()\n",
    "    if not os.path.exists(\"logs\"):\n",
    "        try:\n",
    "            os.makedirs(\"logs\", exist_ok=True)\n",
    "        except BaseException:\n",
    "            pass\n",
    "    for table in [\n",
    "        \"chat_completions\",\n",
    "        \"agents\",\n",
    "        \"oai_wrappers\",\n",
    "        \"oai_clients\",\n",
    "        \"version\",\n",
    "        \"events\",\n",
    "        \"function_calls\",\n",
    "    ]:\n",
    "        dest = os.path.join(\"logs\", f\"{table}.csv\")\n",
    "        get_sqlite_out(\"flow.db\", table, dest)\n",
    "\n",
    "\n",
    "def _check_for_extra_agents(agent: ConversableAgent) -> list[ConversableAgent]:\n",
    "    _extra_agents: list[ConversableAgent] = []\n",
    "    _agent_cls_name = agent.__class__.__name__\n",
    "    if _agent_cls_name == \"CaptainAgent\":\n",
    "        _assistant_agent = getattr(agent, \"assistant\", None)\n",
    "        if _assistant_agent and _assistant_agent not in _extra_agents:\n",
    "            _extra_agents.append(_assistant_agent)\n",
    "        _executor_agent = getattr(agent, \"executor\", None)\n",
    "        if _executor_agent and _executor_agent not in _extra_agents:\n",
    "            _extra_agents.append(_executor_agent)\n",
    "    return _extra_agents\n",
    "\n",
    "\n",
    "def _check_for_group_members(agent: ConversableAgent) -> list[ConversableAgent]:\n",
    "    _extra_agents: list[ConversableAgent] = []\n",
    "    _group_chat = getattr(agent, \"_groupchat\", None)\n",
    "    if _group_chat:\n",
    "        _chat_agents = getattr(_group_chat, \"agents\", [])\n",
    "        if isinstance(_chat_agents, list):\n",
    "            for _group_member in _chat_agents:\n",
    "                if _group_member not in _extra_agents:\n",
    "                    _extra_agents.append(_group_member)\n",
    "    _manager = getattr(agent, \"_group_manager\", None)\n",
    "    if _manager:\n",
    "        if _manager not in _extra_agents:\n",
    "            _extra_agents.append(_manager)\n",
    "        for _group_member in _check_for_group_members(_manager):\n",
    "            if _group_member not in _extra_agents:\n",
    "                _extra_agents.append(_group_member)\n",
    "    return _extra_agents\n",
    "\n",
    "\n",
    "def _get_known_agents() -> list[ConversableAgent]:\n",
    "    _known_agents: list[ConversableAgent] = []\n",
    "    if User not in _known_agents:\n",
    "        _known_agents.append(User)\n",
    "    _known_agents.append(User)\n",
    "    for _group_member in _check_for_group_members(User):\n",
    "        if _group_member not in _known_agents:\n",
    "            _known_agents.append(_group_member)\n",
    "    for _extra_agent in _check_for_extra_agents(User):\n",
    "        if _extra_agent not in _known_agents:\n",
    "            _known_agents.append(_extra_agent)\n",
    "\n",
    "    if taskmaster_agent not in _known_agents:\n",
    "        _known_agents.append(taskmaster_agent)\n",
    "    _known_agents.append(taskmaster_agent)\n",
    "    for _group_member in _check_for_group_members(taskmaster_agent):\n",
    "        if _group_member not in _known_agents:\n",
    "            _known_agents.append(_group_member)\n",
    "    for _extra_agent in _check_for_extra_agents(taskmaster_agent):\n",
    "        if _extra_agent not in _known_agents:\n",
    "            _known_agents.append(_extra_agent)\n",
    "\n",
    "    if evaluator_agent not in _known_agents:\n",
    "        _known_agents.append(evaluator_agent)\n",
    "    _known_agents.append(evaluator_agent)\n",
    "    for _group_member in _check_for_group_members(evaluator_agent):\n",
    "        if _group_member not in _known_agents:\n",
    "            _known_agents.append(_group_member)\n",
    "    for _extra_agent in _check_for_extra_agents(evaluator_agent):\n",
    "        if _extra_agent not in _known_agents:\n",
    "            _known_agents.append(_extra_agent)\n",
    "\n",
    "    if agent_a not in _known_agents:\n",
    "        _known_agents.append(agent_a)\n",
    "    _known_agents.append(agent_a)\n",
    "    for _group_member in _check_for_group_members(agent_a):\n",
    "        if _group_member not in _known_agents:\n",
    "            _known_agents.append(_group_member)\n",
    "    for _extra_agent in _check_for_extra_agents(agent_a):\n",
    "        if _extra_agent not in _known_agents:\n",
    "            _known_agents.append(_extra_agent)\n",
    "\n",
    "    if agent_b not in _known_agents:\n",
    "        _known_agents.append(agent_b)\n",
    "    _known_agents.append(agent_b)\n",
    "    for _group_member in _check_for_group_members(agent_b):\n",
    "        if _group_member not in _known_agents:\n",
    "            _known_agents.append(_group_member)\n",
    "    for _extra_agent in _check_for_extra_agents(agent_b):\n",
    "        if _extra_agent not in _known_agents:\n",
    "            _known_agents.append(_extra_agent)\n",
    "\n",
    "    if agent_c not in _known_agents:\n",
    "        _known_agents.append(agent_c)\n",
    "    _known_agents.append(agent_c)\n",
    "    for _group_member in _check_for_group_members(agent_c):\n",
    "        if _group_member not in _known_agents:\n",
    "            _known_agents.append(_group_member)\n",
    "    for _extra_agent in _check_for_extra_agents(agent_c):\n",
    "        if _extra_agent not in _known_agents:\n",
    "            _known_agents.append(_extra_agent)\n",
    "    return _known_agents\n",
    "\n",
    "\n",
    "def store_error(exc: BaseException | None = None) -> None:\n",
    "    \"\"\"Store the error in error.json.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exc : BaseException | None\n",
    "        The exception we got if any.\n",
    "    \"\"\"\n",
    "    reason = (\n",
    "        \"Event handler stopped processing\"\n",
    "        if not exc\n",
    "        else traceback.format_exc()\n",
    "    )\n",
    "    try:\n",
    "        with open(\"error.json\", \"w\", encoding=\"utf-8\", newline=\"\\n\") as file:\n",
    "            file.write(json.dumps({\"error\": reason}))\n",
    "    except BaseException:  # pylint: disable=broad-exception-caught\n",
    "        pass\n",
    "\n",
    "\n",
    "def store_results(result_dicts: list[dict[str, Any]]) -> None:\n",
    "    \"\"\"Store the results to results.json.\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_dicts : list[dict[str, Any]]\n",
    "        The list of the results.\n",
    "    \"\"\"\n",
    "    with open(\"results.json\", \"w\", encoding=\"utf-8\", newline=\"\\n\") as file:\n",
    "        file.write(\n",
    "            json.dumps({'results': result_dicts}, indent=4, ensure_ascii=False)\n",
    "        )\n",
    "\n",
    "\n",
    "def _get_agent_by_name(\n",
    "    agents: list[ConversableAgent], agent_name: str\n",
    ") -> tuple[int, ConversableAgent | None]:\n",
    "    \"\"\"Get an agent by its name.\"\"\"\n",
    "    for ind, agent in enumerate(agents):\n",
    "        if agent.name == agent_name:\n",
    "            return ind, agent\n",
    "    return -1, None\n",
    "\n",
    "\n",
    "def _handle_resume_group_pattern(\n",
    "    detected_pattern: Pattern, state_messages: list[dict[str, Any]]\n",
    ") -> None:\n",
    "    \"\"\"Handle detected pattern for resuming a group chat.\"\"\"\n",
    "    # pylint: disable=broad-exception-caught,too-many-try-statements\n",
    "    try:\n",
    "        _pattern_type = getattr(detected_pattern.__class__, \"__name__\", None)\n",
    "    except BaseException:\n",
    "        return\n",
    "    if _pattern_type == \"RoundRobinPattern\" and state_messages:\n",
    "        last_message = state_messages[-1]\n",
    "        if not last_message or not isinstance(last_message, dict):\n",
    "            return\n",
    "        last_agent_name = last_message.get(\"name\", \"\")\n",
    "        if not last_agent_name:\n",
    "            return\n",
    "        try:\n",
    "            idx, last_agent = _get_agent_by_name(\n",
    "                detected_pattern.agents, last_agent_name\n",
    "            )\n",
    "            if last_agent and len(detected_pattern.agents) >= (idx + 1):\n",
    "                detected_pattern.agents.append(detected_pattern.user_agent)\n",
    "                detected_pattern.initial_agent = detected_pattern.agents[\n",
    "                    idx + 1\n",
    "                ]\n",
    "                detected_pattern.user_agent = detected_pattern.agents[idx]\n",
    "                # fmt: off\n",
    "                new_agent_order_list = detected_pattern.agents[idx+1:] + detected_pattern.agents[:idx]\n",
    "                # fmt: on\n",
    "                detected_pattern.agents = new_agent_order_list\n",
    "        except BaseException:\n",
    "            pass\n",
    "\n",
    "\n",
    "def _prepare_resume(state_json: str | Path | None = None) -> None:\n",
    "    \"\"\"Prepare resuming a chat from state.json.\n",
    "\n",
    "    state.json format:\n",
    "        {\n",
    "            \"messages\": [{\"content\": \"..\", \"role\": \"...\", \"name\": \"...\"}],\n",
    "            \"context_variables\": {\"key1\": \"value1\", \"key2\": 4, \"key3\": [], \"key4\": {\"other\": \"key\"}}\n",
    "        }\n",
    "    metadata.json format:\n",
    "        {\n",
    "            \"type\": \"group\",\n",
    "            \"group\": {  # one of:\n",
    "                \"pattern\" : \"<pattern_name>\",\n",
    "                \"manager\" : \"<manager_name>\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_json : str | Path | None\n",
    "        The path to state.json to load previous state.\n",
    "    \"\"\"\n",
    "    # pylint: disable=broad-exception-caught,too-many-try-statements,global-statement\n",
    "    global __INITIAL_MSG__\n",
    "    if not state_json or not Path(state_json).is_file():\n",
    "        return\n",
    "    metadata_json = str(state_json).replace(\"state.json\", \"metadata.json\")\n",
    "    if not metadata_json or not Path(metadata_json).is_file():\n",
    "        return\n",
    "    try:\n",
    "        with open(metadata_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            _metadata_dict = json.load(f)\n",
    "    except BaseException:\n",
    "        return\n",
    "    if not _metadata_dict or not isinstance(_metadata_dict, dict):\n",
    "        return\n",
    "    _state_chat_type = _metadata_dict.get(\"type\", \"\")\n",
    "    if _state_chat_type != \"group\":\n",
    "        # only resume group chats\n",
    "        return\n",
    "    _state_group_details = _metadata_dict.get(\"group\", {})\n",
    "    if not _state_group_details or not isinstance(_state_group_details, dict):\n",
    "        return\n",
    "    # either pattern or manager\n",
    "    _state_group_pattern = _state_group_details.get(\"pattern\", \"\")\n",
    "    _state_group_manager = _state_group_details.get(\"manager\", \"\")\n",
    "    if not _state_group_pattern and not _state_group_manager:\n",
    "        return\n",
    "    try:\n",
    "        with open(state_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            _state_dict = json.load(f)\n",
    "    except BaseException:\n",
    "        return\n",
    "    if not _state_dict or not isinstance(_state_dict, dict):\n",
    "        return\n",
    "    _state_messages = _state_dict.get(\"messages\", [])\n",
    "    _detected_pattern = None\n",
    "    if _state_group_pattern and isinstance(_state_group_pattern, str):\n",
    "        _detected_pattern = __GROUP__[\"patterns\"].get(\n",
    "            _state_group_pattern, None\n",
    "        )\n",
    "        if _detected_pattern:\n",
    "            _state_context_variables = _state_dict.get(\"context_variables\", {})\n",
    "            if _state_context_variables and isinstance(\n",
    "                _state_context_variables, dict\n",
    "            ):\n",
    "                _new_context_variables = (\n",
    "                    _detected_pattern.context_variables.data.copy()\n",
    "                )\n",
    "                _new_context_variables.update(_state_context_variables)\n",
    "                _detected_pattern.context_variables = ContextVariables(\n",
    "                    data=_new_context_variables\n",
    "                )\n",
    "        if _state_messages and isinstance(_state_messages, list):\n",
    "            __INITIAL_MSG__ = _state_messages\n",
    "    elif _state_group_manager and isinstance(_state_group_manager, str):\n",
    "        _known_group_manager = __AGENTS__.get(_state_group_manager, None)\n",
    "        if _known_group_manager and hasattr(_known_group_manager, \"groupchat\"):\n",
    "            if _state_messages and isinstance(_state_messages, list):\n",
    "                _known_group_manager.groupchat.messages = _state_messages\n",
    "        else:\n",
    "            _detected_pattern = __GROUP__[\"patterns\"].get(\n",
    "                f\"{_state_group_manager}_pattern\"\n",
    "            )\n",
    "            if _detected_pattern:\n",
    "                _state_context_variables = _state_dict.get(\n",
    "                    \"context_variables\", {}\n",
    "                )\n",
    "                if _state_context_variables and isinstance(\n",
    "                    _state_context_variables, dict\n",
    "                ):\n",
    "                    _new_context_variables = (\n",
    "                        _detected_pattern.context_variables.data.copy()\n",
    "                    )\n",
    "                    _new_context_variables.update(_state_context_variables)\n",
    "                    _detected_pattern.context_variables = ContextVariables(\n",
    "                        data=_new_context_variables\n",
    "                    )\n",
    "            if _state_messages and isinstance(_state_messages, list):\n",
    "                __INITIAL_MSG__ = _state_messages\n",
    "    if (\n",
    "        _detected_pattern\n",
    "        and _state_messages\n",
    "        and isinstance(_state_messages, list)\n",
    "    ):\n",
    "        _handle_resume_group_pattern(_detected_pattern, _state_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16440999",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f657414",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    on_event: Callable[[BaseEvent, list[ConversableAgent]], bool] | None = None,\n",
    "    state_json: str | Path | None = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Start chatting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        The result of the chat session.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    SystemExit\n",
    "        If the user interrupts the chat session.\n",
    "    \"\"\"\n",
    "    if state_json:\n",
    "        _prepare_resume(state_json)\n",
    "    results: list[RunResponseProtocol] | RunResponseProtocol = []\n",
    "    result_dicts: list[dict[str, Any]] = []\n",
    "    a_pause_event = asyncio.Event()\n",
    "    a_pause_event.set()\n",
    "    pause_event = threading.Event()\n",
    "    pause_event.set()\n",
    "    if Path(\".cache\").is_dir():\n",
    "        shutil.rmtree(\".cache\", ignore_errors=True)\n",
    "    results = run_group_chat(\n",
    "        pattern=_pattern,\n",
    "        messages=__INITIAL_MSG__,\n",
    "        max_rounds=30,\n",
    "        pause_event=pause_event,\n",
    "    )\n",
    "    if not isinstance(results, list):\n",
    "        results = [results]  # pylint: disable=redefined-variable-type\n",
    "    got_agents = False\n",
    "    known_agents: list[ConversableAgent] = []\n",
    "    result_events: list[dict[str, Any]] = []\n",
    "    if on_event:\n",
    "        for index, result in enumerate(results):\n",
    "            result_events = []\n",
    "            for event in result.events:\n",
    "                try:\n",
    "                    result_events.append(\n",
    "                        event.model_dump(mode=\"json\", fallback=str)\n",
    "                    )\n",
    "                except BaseException:  # pylint: disable=broad-exception-caught\n",
    "                    pass\n",
    "                if not got_agents:\n",
    "                    known_agents = _get_known_agents()\n",
    "                    got_agents = True\n",
    "                pause_event.clear()\n",
    "                try:\n",
    "                    should_continue = on_event(event, known_agents)\n",
    "                    pause_event.set()\n",
    "                except BaseException as e:\n",
    "                    stop_logging()\n",
    "                    store_error(e)\n",
    "                    raise SystemExit(\"Error in event handler: \" + str(e)) from e\n",
    "                if getattr(event, \"type\") == \"run_completion\":\n",
    "                    break\n",
    "                if not should_continue:\n",
    "                    stop_logging()\n",
    "                    store_error()\n",
    "                    raise SystemExit(\"Event handler stopped processing\")\n",
    "            result_cost = result.cost\n",
    "            result_context_variables = result.context_variables\n",
    "            result_dict = {\n",
    "                \"index\": index,\n",
    "                \"uuid\": str(result.uuid),\n",
    "                \"events\": result_events,\n",
    "                \"messages\": result.messages,\n",
    "                \"summary\": result.summary,\n",
    "                \"cost\": (\n",
    "                    result_cost.model_dump(mode=\"json\", fallback=str)\n",
    "                    if result_cost\n",
    "                    else None\n",
    "                ),\n",
    "                \"context_variables\": (\n",
    "                    result_context_variables.model_dump(\n",
    "                        mode=\"json\", fallback=str\n",
    "                    )\n",
    "                    if result_context_variables\n",
    "                    else None\n",
    "                ),\n",
    "                \"last_speaker\": result.last_speaker,\n",
    "            }\n",
    "            result_dicts.append(result_dict)\n",
    "    else:\n",
    "        for index, result in enumerate(results):\n",
    "            result_events = []\n",
    "            # result.process()\n",
    "            for event in result.events:\n",
    "                try:\n",
    "                    result_events.append(\n",
    "                        event.model_dump(mode=\"json\", fallback=str)\n",
    "                    )\n",
    "                except BaseException:  # pylint: disable=broad-exception-caught\n",
    "                    pass\n",
    "            result_cost = result.cost\n",
    "            result_context_variables = result.context_variables\n",
    "            result_dict = {\n",
    "                \"index\": index,\n",
    "                \"uuid\": str(result.uuid),\n",
    "                \"events\": result_events,\n",
    "                \"messages\": result.messages,\n",
    "                \"summary\": result.summary,\n",
    "                \"cost\": (\n",
    "                    result_cost.model_dump(mode=\"json\", fallback=str)\n",
    "                    if result_cost\n",
    "                    else None\n",
    "                ),\n",
    "                \"context_variables\": (\n",
    "                    result_context_variables.model_dump(\n",
    "                        mode=\"json\", fallback=str\n",
    "                    )\n",
    "                    if result_context_variables\n",
    "                    else None\n",
    "                ),\n",
    "                \"last_speaker\": result.last_speaker,\n",
    "            }\n",
    "            result_dicts.append(result_dict)\n",
    "\n",
    "    stop_logging()\n",
    "    store_results(result_dicts)\n",
    "    return result_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": false,
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
