{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e825690d",
   "metadata": {},
   "source": [
    "# Name: AutoDefense Flow\n",
    "\n",
    "## Description: Multi-Agent LLM Defense against Jailbreak Attacks\n",
    "\n",
    "## Tags: AutoDefense, Jailbreak\n",
    "\n",
    "###ðŸ§© generated with â¤ï¸ by Waldiez.\n",
    "\n",
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586b5b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys  # pyright: ignore\n",
    "\n",
    "# # !{sys.executable} -m pip install -q ag2[openai]==0.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbfe11",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce8d18",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnusedImport=false,reportMissingTypeStubs=false\n",
    "import csv\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import sys\n",
    "from dataclasses import asdict\n",
    "from pprint import pprint\n",
    "from types import ModuleType\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import autogen  # type: ignore\n",
    "from autogen import (\n",
    "    Agent,\n",
    "    AssistantAgent,\n",
    "    Cache,\n",
    "    ChatResult,\n",
    "    ConversableAgent,\n",
    "    GroupChat,\n",
    "    UserProxyAgent,\n",
    "    runtime_logging,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# let's try to avoid:\n",
    "# module 'numpy' has no attribute '_no_nep50_warning'\"\n",
    "# ref: https://github.com/numpy/numpy/blob/v2.2.2/doc/source/release/2.2.0-notes.rst#nep-50-promotion-state-option-removed\n",
    "os.environ[\"NEP50_DEPRECATION_WARNING\"] = \"0\"\n",
    "os.environ[\"NEP50_DISABLE_WARNING\"] = \"1\"\n",
    "os.environ[\"NPY_PROMOTION_STATE\"] = \"weak\"\n",
    "if not hasattr(np, \"_no_pep50_warning\"):\n",
    "\n",
    "    import contextlib\n",
    "    from typing import Generator\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def _np_no_nep50_warning() -> Generator[None, None, None]:\n",
    "        \"\"\"Dummy function to avoid the warning.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        None\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "        yield\n",
    "\n",
    "    setattr(np, \"_no_pep50_warning\", _np_no_nep50_warning)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed311413",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197df00",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def start_logging() -> None:\n",
    "    \"\"\"Start logging.\"\"\"\n",
    "    runtime_logging.start(\n",
    "        logger_type=\"sqlite\",\n",
    "        config={\"dbname\": \"flow.db\"},\n",
    "    )\n",
    "\n",
    "\n",
    "start_logging()\n",
    "\n",
    "# patch the default IOStream\n",
    "try:\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    from waldiez.running.patch_io_stream import patch_io_stream\n",
    "\n",
    "    patch_io_stream(is_async=False)\n",
    "except BaseException:  # pylint: disable=broad-exception-caught\n",
    "    # allow running the flow without patching the IOStream\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0839089",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Load model API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af89ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# This section assumes that a file named \"autodefense_flow_api_keys\"\n",
    "# exists in the same directory as this file.\n",
    "# This file contains the API keys for the models used in this flow.\n",
    "# It should be .gitignored and not shared publicly.\n",
    "# If this file is not present, you can either create it manually\n",
    "# or change the way API keys are loaded in the flow.\n",
    "\n",
    "\n",
    "def load_api_key_module(flow_name: str) -> ModuleType:\n",
    "    \"\"\"Load the api key module.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flow_name : str\n",
    "        The flow name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ModuleType\n",
    "        The api keys loading module.\n",
    "    \"\"\"\n",
    "    module_name = f\"{flow_name}_api_keys\"\n",
    "    if module_name in sys.modules:\n",
    "        return importlib.reload(sys.modules[module_name])\n",
    "    return importlib.import_module(module_name)\n",
    "\n",
    "\n",
    "__MODELS_MODULE__ = load_api_key_module(\"autodefense_flow\")\n",
    "\n",
    "\n",
    "def get_autodefense_flow_model_api_key(model_name: str) -> str:\n",
    "    \"\"\"Get the model api key.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        The model name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The model api key.\n",
    "    \"\"\"\n",
    "    return __MODELS_MODULE__.get_autodefense_flow_model_api_key(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aac5eb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f75724",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_3_5_turbo_llm_config: dict[str, Any] = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"api_type\": \"openai\",\n",
    "    \"api_key\": get_autodefense_flow_model_api_key(\"gpt_3_5_turbo\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02c65f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnnecessaryIsInstance=false\n",
    "\n",
    "answer_provider = AssistantAgent(\n",
    "    name=\"answer_provider\",\n",
    "    description=\"Final answer provider\",\n",
    "    system_message=\"Provide an answer to the original user's question. Consider the content provided by the other agents. In case the question the judge think it is harmful just say: \\\"I'm sorry, but I cannot fulfill this request.\\\" otherwise answer the questions using two sentences.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_3_5_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "intention_analyzer = AssistantAgent(\n",
    "    name=\"intention_analyzer\",\n",
    "    description=\"Intention Analyzer\",\n",
    "    system_message=\"Your role is to analyze the given content and present a comprehensive analysis\\nof the intended message behind the text, including its context and any implied meanings.\\nYou should present a fair argument without judgment.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_3_5_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "interface = AssistantAgent(\n",
    "    name=\"interface\",\n",
    "    description=\"You are just the input and output of the guardrail system\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_3_5_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "judge = AssistantAgent(\n",
    "    name=\"judge\",\n",
    "    description=\"Judge\",\n",
    "    system_message=\"Your role is to evaluate the response presented by Intention Analyzer and Original Prompt Analyzer. Consider the intention and prompt inference to deliver a judgment on whether the system input content is valid or invalid.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_3_5_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "original_prompt_analyzer = AssistantAgent(\n",
    "    name=\"original_prompt_analyzer\",\n",
    "    description=\"Original Prompt Analyzer\",\n",
    "    system_message=\"Your task is to infer the original prompt that led to the given LLM output, you should present three possible prompts that may produce this output. Please use the context provided by Intention Analyzer.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_3_5_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "user = UserProxyAgent(\n",
    "    name=\"user\",\n",
    "    description=\"A new User proxy agent\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    llm_config=False,  # pyright: ignore\n",
    ")\n",
    "\n",
    "\n",
    "def nested_chat_message_analyzeintent(\n",
    "    recipient: ConversableAgent,\n",
    "    messages: list[dict[str, Any]],\n",
    "    sender: ConversableAgent,\n",
    "    config: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Ask for a review.\"\"\"\n",
    "    return f\"\"\"Review the following content.\n",
    "{recipient.chat_messages_for_summary(sender)[-1]['content']}\"\"\"\n",
    "\n",
    "\n",
    "def nested_chat_message_analyzeoriginalprompt(\n",
    "    recipient: ConversableAgent,\n",
    "    messages: list[dict[str, Any]],\n",
    "    sender: ConversableAgent,\n",
    "    config: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Ask for a review.\"\"\"\n",
    "    return f\"\"\"Review the following content.\n",
    "{recipient.chat_messages_for_summary(sender)[-1]['content']}\"\"\"\n",
    "\n",
    "\n",
    "def nested_chat_message_judgecontent(\n",
    "    recipient: ConversableAgent,\n",
    "    messages: list[dict[str, Any]],\n",
    "    sender: ConversableAgent,\n",
    "    config: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Ask for a review.\"\"\"\n",
    "    return f\"\"\"Review the following content.\n",
    "{recipient.chat_messages_for_summary(sender)[-1]['content']}\"\"\"\n",
    "\n",
    "\n",
    "interface_chat_queue: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"summary_method\": \"last_msg\",\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": True,\n",
    "        \"chat_id\": 0,\n",
    "        \"recipient\": intention_analyzer,\n",
    "        \"message\": nested_chat_message_analyzeintent,\n",
    "    },\n",
    "    {\n",
    "        \"summary_method\": \"last_msg\",\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": True,\n",
    "        \"chat_id\": 1,\n",
    "        \"recipient\": original_prompt_analyzer,\n",
    "        \"message\": nested_chat_message_analyzeoriginalprompt,\n",
    "    },\n",
    "    {\n",
    "        \"summary_method\": \"last_msg\",\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": True,\n",
    "        \"chat_id\": 2,\n",
    "        \"recipient\": judge,\n",
    "        \"message\": nested_chat_message_judgecontent,\n",
    "    },\n",
    "    {\n",
    "        \"summary_method\": \"last_msg\",\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\": True,\n",
    "        \"chat_id\": 3,\n",
    "        \"recipient\": answer_provider,\n",
    "        \"message\": \"Aggregate feedback from all reviewers and give final suggestions on the writing.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "interface.register_nested_chats(  # pyright: ignore\n",
    "    trigger=[\"user\"],\n",
    "    chat_queue=interface_chat_queue,\n",
    "    use_async=False,\n",
    "    ignore_async_in_sync_chat=True,\n",
    ")\n",
    "\n",
    "\n",
    "def get_sqlite_out(dbname: str, table: str, csv_file: str) -> None:\n",
    "    \"\"\"Convert a sqlite table to csv and json files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dbname : str\n",
    "        The sqlite database name.\n",
    "    table : str\n",
    "        The table name.\n",
    "    csv_file : str\n",
    "        The csv file name.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    query = f\"SELECT * FROM {table}\"  # nosec\n",
    "    try:\n",
    "        cursor = conn.execute(query)\n",
    "    except sqlite3.OperationalError:\n",
    "        conn.close()\n",
    "        return\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    data = [dict(zip(column_names, row)) for row in rows]\n",
    "    conn.close()\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        csv_writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(data)\n",
    "    json_file = csv_file.replace(\".csv\", \".json\")\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def stop_logging() -> None:\n",
    "    \"\"\"Stop logging.\"\"\"\n",
    "    runtime_logging.stop()\n",
    "    if not os.path.exists(\"logs\"):\n",
    "        os.makedirs(\"logs\")\n",
    "    for table in [\n",
    "        \"chat_completions\",\n",
    "        \"agents\",\n",
    "        \"oai_wrappers\",\n",
    "        \"oai_clients\",\n",
    "        \"version\",\n",
    "        \"events\",\n",
    "        \"function_calls\",\n",
    "    ]:\n",
    "        dest = os.path.join(\"logs\", f\"{table}.csv\")\n",
    "        get_sqlite_out(\"flow.db\", table, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2252e0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87e8c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main() -> Union[ChatResult, list[ChatResult], dict[int, ChatResult]]:\n",
    "    \"\"\"Start chatting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[ChatResult, list[ChatResult], dict[int, ChatResult]]\n",
    "        The result of the chat session, which can be a single ChatResult,\n",
    "        a list of ChatResults, or a dictionary mapping integers to ChatResults.\n",
    "    \"\"\"\n",
    "    with Cache.disk(cache_seed=42) as cache:  # pyright: ignore\n",
    "        results = interface.initiate_chat(\n",
    "            user,\n",
    "            cache=cache,\n",
    "            summary_method=\"last_msg\",\n",
    "            max_turns=3,\n",
    "            clear_history=True,\n",
    "            message=\"Hi there, how I can assist you?\",\n",
    "        )\n",
    "\n",
    "        stop_logging()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c152239",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": false,
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
