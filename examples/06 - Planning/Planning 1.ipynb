{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c55c0bc",
   "metadata": {},
   "source": [
    "# Name: Planning 1\n",
    "\n",
    "## Description: Planning and Stock Report Generation\n",
    "\n",
    "## Tags: Planning, Stock report, Group\n",
    "\n",
    "###ðŸ§© generated with â¤ï¸ by Waldiez.\n",
    "\n",
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf42e23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys  # pyright: ignore\n",
    "\n",
    "# # !{sys.executable} -m pip install -q ag2[openai]==0.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706f34f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9eccf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnusedImport=false,reportMissingTypeStubs=false\n",
    "import csv\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import sys\n",
    "from dataclasses import asdict\n",
    "from pprint import pprint\n",
    "from types import ModuleType\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import autogen  # type: ignore\n",
    "from autogen import (\n",
    "    Agent,\n",
    "    Cache,\n",
    "    ChatResult,\n",
    "    ConversableAgent,\n",
    "    GroupChat,\n",
    "    UserProxyAgent,\n",
    "    runtime_logging,\n",
    ")\n",
    "from autogen.agentchat import GroupChatManager, initiate_group_chat\n",
    "from autogen.agentchat.group import ContextVariables\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# let's try to avoid:\n",
    "# module 'numpy' has no attribute '_no_nep50_warning'\"\n",
    "# ref: https://github.com/numpy/numpy/blob/v2.2.2/doc/source/release/2.2.0-notes.rst#nep-50-promotion-state-option-removed\n",
    "os.environ[\"NEP50_DEPRECATION_WARNING\"] = \"0\"\n",
    "os.environ[\"NEP50_DISABLE_WARNING\"] = \"1\"\n",
    "os.environ[\"NPY_PROMOTION_STATE\"] = \"weak\"\n",
    "if not hasattr(np, \"_no_pep50_warning\"):\n",
    "\n",
    "    import contextlib\n",
    "    from typing import Generator\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def _np_no_nep50_warning() -> Generator[None, None, None]:\n",
    "        \"\"\"Dummy function to avoid the warning.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        None\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "        yield\n",
    "\n",
    "    setattr(np, \"_no_pep50_warning\", _np_no_nep50_warning)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1279ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logging() -> None:\n",
    "    \"\"\"Start logging.\"\"\"\n",
    "    runtime_logging.start(\n",
    "        logger_type=\"sqlite\",\n",
    "        config={\"dbname\": \"flow.db\"},\n",
    "    )\n",
    "\n",
    "\n",
    "start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a4482",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Load model API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd256725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# This section assumes that a file named \"planning_1_api_keys\"\n",
    "# exists in the same directory as this file.\n",
    "# This file contains the API keys for the models used in this flow.\n",
    "# It should be .gitignored and not shared publicly.\n",
    "# If this file is not present, you can either create it manually\n",
    "# or change the way API keys are loaded in the flow.\n",
    "\n",
    "\n",
    "def load_api_key_module(flow_name: str) -> ModuleType:\n",
    "    \"\"\"Load the api key module.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flow_name : str\n",
    "        The flow name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ModuleType\n",
    "        The api keys loading module.\n",
    "    \"\"\"\n",
    "    module_name = f\"{flow_name}_api_keys\"\n",
    "    if module_name in sys.modules:\n",
    "        return importlib.reload(sys.modules[module_name])\n",
    "    return importlib.import_module(module_name)\n",
    "\n",
    "\n",
    "__MODELS_MODULE__ = load_api_key_module(\"planning_1\")\n",
    "\n",
    "\n",
    "def get_planning_1_model_api_key(model_name: str) -> str:\n",
    "    \"\"\"Get the model api key.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        The model name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The model api key.\n",
    "    \"\"\"\n",
    "    return __MODELS_MODULE__.get_planning_1_model_api_key(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a5b64",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_turbo_llm_config: dict[str, Any] = {\n",
    "    \"model\": \"gpt-4-turbo\",\n",
    "    \"api_type\": \"openai\",\n",
    "    \"api_key\": get_planning_1_model_api_key(\"gpt_4_turbo\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9828b828",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35419931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnnecessaryIsInstance=false\n",
    "\n",
    "executor_executor = LocalCommandLineCodeExecutor(\n",
    "    work_dir=\"coding\",\n",
    "    timeout=60,\n",
    ")\n",
    "\n",
    "engineer = ConversableAgent(\n",
    "    name=\"engineer\",\n",
    "    description=\"An engineer that writes code based on the plan provided by the planner.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    functions=[],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "executor = ConversableAgent(\n",
    "    name=\"executor\",\n",
    "    description=\"Executor agent\",\n",
    "    system_message=\"Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config={\"executor\": executor_executor},\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    functions=[],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=False,  # pyright: ignore\n",
    ")\n",
    "\n",
    "planner = ConversableAgent(\n",
    "    name=\"planner\",\n",
    "    description=\"Planner agent\",\n",
    "    system_message=\"Given a task, please determine what information is needed to complete the task. Please note that the information will all be retrieved using Python code. Please only suggest information that can be retrieved using Python code. After each step is done by others, check the progress and instruct the remaining steps. If a step fails, try to workaround.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    functions=[],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    description=\"A new User proxy agent\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    llm_config=False,  # pyright: ignore\n",
    ")\n",
    "\n",
    "writer = ConversableAgent(\n",
    "    name=\"writer\",\n",
    "    description=\"Writer agent\",\n",
    "    system_message=\"Writer. Please write blogs in markdown format (with relevant titles) and put the content in pseudo ```md``` code block. You take feedback from the admin and refine your blog.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    functions=[],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "manager_group_chat = GroupChat(\n",
    "    agents=[planner, engineer, executor, writer, user_proxy],\n",
    "    enable_clear_history=False,\n",
    "    send_introductions=False,\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    admin_name=\"user_proxy\",\n",
    "    max_retries_for_selecting_speaker=10,\n",
    "    speaker_selection_method=\"auto\",\n",
    "    allow_repeat_speaker=True,\n",
    ")\n",
    "\n",
    "\n",
    "def callable_message_user_proxy_to_manager(\n",
    "    sender: ConversableAgent,\n",
    "    recipient: ConversableAgent,\n",
    "    context: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Complete the message function\"\"\"\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    import datetime\n",
    "\n",
    "    today = datetime.datetime.now().date()\n",
    "    message = (\n",
    "        \"Write a blogpost about the stock price performance of \"\n",
    "        f\"Nvidia in the past month. Today's date is {today}\"\n",
    "    )\n",
    "    return message\n",
    "\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    name=\"manager\",\n",
    "    description=\"The group manager agent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    groupchat=manager_group_chat,\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_turbo_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def get_sqlite_out(dbname: str, table: str, csv_file: str) -> None:\n",
    "    \"\"\"Convert a sqlite table to csv and json files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dbname : str\n",
    "        The sqlite database name.\n",
    "    table : str\n",
    "        The table name.\n",
    "    csv_file : str\n",
    "        The csv file name.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    query = f\"SELECT * FROM {table}\"  # nosec\n",
    "    try:\n",
    "        cursor = conn.execute(query)\n",
    "    except sqlite3.OperationalError:\n",
    "        conn.close()\n",
    "        return\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    data = [dict(zip(column_names, row)) for row in rows]\n",
    "    conn.close()\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        csv_writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(data)\n",
    "    json_file = csv_file.replace(\".csv\", \".json\")\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def stop_logging() -> None:\n",
    "    \"\"\"Stop logging.\"\"\"\n",
    "    runtime_logging.stop()\n",
    "    if not os.path.exists(\"logs\"):\n",
    "        os.makedirs(\"logs\")\n",
    "    for table in [\n",
    "        \"chat_completions\",\n",
    "        \"agents\",\n",
    "        \"oai_wrappers\",\n",
    "        \"oai_clients\",\n",
    "        \"version\",\n",
    "        \"events\",\n",
    "        \"function_calls\",\n",
    "    ]:\n",
    "        dest = os.path.join(\"logs\", f\"{table}.csv\")\n",
    "        get_sqlite_out(\"flow.db\", table, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820a79c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288419db",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main() -> Union[ChatResult, list[ChatResult], dict[int, ChatResult]]:\n",
    "    \"\"\"Start chatting.\n",
    "    Returns\n",
    "    -------\n",
    "    Union[ChatResult, list[ChatResult], dict[int, ChatResult]]\n",
    "        The result of the chat session, which can be a single ChatResult,\n",
    "        a list of ChatResults, or a dictionary mapping integers to ChatResults.\n",
    "    \"\"\"\n",
    "    with Cache.disk(cache_seed=42) as cache:  # pyright: ignore\n",
    "        results = user_proxy.initiate_chat(\n",
    "            manager,\n",
    "            cache=cache,\n",
    "            summary_method=\"last_msg\",\n",
    "            clear_history=True,\n",
    "            message=callable_message_user_proxy_to_manager,\n",
    "        )\n",
    "\n",
    "        stop_logging()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9693c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": false,
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
