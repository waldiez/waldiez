diff --git a/autogen/agentchat/conversable_agent.py b/autogen/agentchat/conversable_agent.py
index 5ed5e6bc2a..d84d7ff2e9 100644
--- a/autogen/agentchat/conversable_agent.py
+++ b/autogen/agentchat/conversable_agent.py
@@ -78,6 +78,8 @@
 from .agent import Agent, LLMAgent
 from .chat import (
     ChatResult,
+    __create_async_prerequisites,
+    __find_async_chat_order,
     _post_process_carryover_item,
     _validate_recipients,
     a_initiate_chats,
@@ -2348,54 +2350,94 @@ async def a_sequential_run(
         # todo: add agents
         responses = [AsyncRunResponse(iostream, agents=[]) for iostream in iostreams]
 
-        async def _a_initiate_chats(
-            iostreams: list[AsyncThreadIOStream] = iostreams,
-            responses: list[AsyncRunResponseProtocol] = responses,
-        ) -> None:
-            response = responses[0]
+        async def _process_single_chat(
+            chat_info: dict,
+            response: AsyncRunResponseProtocol,
+            iostream: AsyncThreadIOStream,
+            finished_chats_by_index: dict[int, ChatResult],
+            chat_index: int,
+        ) -> ChatResult:
+            """Process a single chat and return the result."""
+            with IOStream.set_default(iostream):
+                _chat_carryover = chat_info.get("carryover", [])
+                finished_chat_indexes_to_exclude = chat_info.get(
+                    "finished_chat_indexes_to_exclude_from_carryover", []
+                )
+
+                if isinstance(_chat_carryover, str):
+                    _chat_carryover = [_chat_carryover]
+
+                chat_info["carryover"] = _chat_carryover + [
+                    r.summary
+                    for i, r in finished_chats_by_index.items()
+                    if i not in finished_chat_indexes_to_exclude
+                ]
+
+                if not chat_info.get("silent", False):
+                    iostream.send(PostCarryoverProcessingEvent(chat_info=chat_info))
+
+                sender = chat_info["sender"]
+                chat_res = await sender.a_initiate_chat(**chat_info)
+
+                iostream.send(
+                    RunCompletionEvent(
+                        history=chat_res.chat_history,
+                        summary=chat_res.summary,
+                        cost=chat_res.cost,
+                        last_speaker=(self if chat_res.chat_history[-1]["name"] == self.name else sender).name,
+                    )
+                )
+                return chat_res
+
+        async def _a_initiate_chats() -> None:
             try:
                 _chat_queue = self._check_chat_queue_for_sender(chat_queue)
-
                 consolidate_chat_info(_chat_queue)
                 _validate_recipients(_chat_queue)
-                finished_chats = []
-                for chat_info, response, iostream in zip(_chat_queue, responses, iostreams):
-                    with IOStream.set_default(iostream):
-                        _chat_carryover = chat_info.get("carryover", [])
-                        finished_chat_indexes_to_exclude_from_carryover = chat_info.get(
-                            "finished_chat_indexes_to_exclude_from_carryover", []
-                        )
 
-                        if isinstance(_chat_carryover, str):
-                            _chat_carryover = [_chat_carryover]
-                        chat_info["carryover"] = _chat_carryover + [
-                            r.summary
-                            for i, r in enumerate(finished_chats)
-                            if i not in finished_chat_indexes_to_exclude_from_carryover
-                        ]
+                # Build prerequisite graph (use chat index as ID if no chat_id provided)
+                for i, chat_info in enumerate(_chat_queue):
+                    if "chat_id" not in chat_info:
+                        chat_info["chat_id"] = i
 
-                        if not chat_info.get("silent", False):
-                            iostream.send(PostCarryoverProcessingEvent(chat_info=chat_info))
+                prerequisites = __create_async_prerequisites(_chat_queue)
+                chat_ids = [c["chat_id"] for c in _chat_queue]
+                chat_order = __find_async_chat_order(chat_ids, prerequisites)
 
-                        sender = chat_info["sender"]
-                        chat_res = await sender.a_initiate_chat(**chat_info)
+                # Map chat_id to index, chat_info, response, iostream
+                chat_map = {
+                    c["chat_id"]: (i, c, responses[i], iostreams[i])
+                    for i, c in enumerate(_chat_queue)
+                }
 
-                        iostream.send(
-                            RunCompletionEvent(
-                                history=chat_res.chat_history,
-                                summary=chat_res.summary,
-                                cost=chat_res.cost,
-                                last_speaker=(self if chat_res.chat_history[-1]["name"] == self.name else sender).name,
-                            )
-                        )
+                finished_chat_futures: dict[Any, asyncio.Future] = {}
+                finished_chats_by_index: dict[int, ChatResult] = {}
 
-                        finished_chats.append(chat_res)
+                for chat_id in chat_order:
+                    idx, chat_info, response, iostream = chat_map[chat_id]
+                    prerequisite_ids = chat_info.get("prerequisites", [])
+
+                    # Wait for prerequisites to complete
+                    if prerequisite_ids:
+                        await asyncio.gather(*[finished_chat_futures[pid] for pid in prerequisite_ids])
+
+                    # Create task for this chat
+                    async def _run_chat(ci=chat_info, r=response, io=iostream, i=idx):
+                        result = await _process_single_chat(ci, r, io, finished_chats_by_index, i)
+                        finished_chats_by_index[i] = result
+                        return result
+
+                    future = asyncio.create_task(_run_chat())
+                    finished_chat_futures[chat_id] = future
+
+                # Wait for all to complete
+                await asyncio.gather(*finished_chat_futures.values())
 
             except Exception as e:
-                iostream.send(ErrorEvent(error=e))
+                for iostream in iostreams:
+                    iostream.send(ErrorEvent(error=e))
 
         asyncio.create_task(_a_initiate_chats())
-
         return responses
 
     def get_chat_results(self, chat_index: int | None = None) -> list[ChatResult] | ChatResult:
diff --git a/autogen/agentchat/group/multi_agent_chat.py b/autogen/agentchat/group/multi_agent_chat.py
index b1486c25ad..817a590370 100644
--- a/autogen/agentchat/group/multi_agent_chat.py
+++ b/autogen/agentchat/group/multi_agent_chat.py
@@ -48,6 +48,7 @@ def initiate_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    pause_event: threading.Event | None = None,
 ) -> tuple[ChatResult, ContextVariables, "Agent"]:
     """Initialize and run a group chat using a pattern for configuration.
 
@@ -58,12 +59,16 @@ def initiate_group_chat(
         safeguard_policy: Optional safeguard policy dict or path to JSON file.
         safeguard_llm_config: Optional LLM configuration for safeguard checks.
         mask_llm_config: Optional LLM configuration for masking.
+        pause_event: Optional Event to pause the chat.
 
     Returns:
         ChatResult:         Conversations chat history.
         ContextVariables:   Updated Context variables.
         "ConversableAgent":   Last speaker.
     """
+    if pause_event is None:
+        pause_event = threading.Event()
+        pause_event.set()
     # Let the pattern prepare the group chat and all its components
     # Only passing the necessary parameters that aren't already in the pattern
     (
@@ -107,6 +112,8 @@ def initiate_group_chat(
     if last_agent is None:
         raise ValueError("No agent selected to start the conversation")
 
+    manager._pause_event = pause_event
+
     chat_result = last_agent.initiate_chat(
         manager,
         message=last_message,
@@ -133,6 +140,7 @@ async def a_initiate_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    a_pause_event: asyncio.Event | None = None,
 ) -> tuple[ChatResult, ContextVariables, "Agent"]:
     """Initialize and run a group chat using a pattern for configuration, asynchronously.
 
@@ -143,12 +151,16 @@ async def a_initiate_group_chat(
         safeguard_policy: Optional safeguard policy dict or path to JSON file.
         safeguard_llm_config: Optional LLM configuration for safeguard checks.
         mask_llm_config: Optional LLM configuration for masking.
+        a_pause_event: Optional Event to pause the chat.
 
     Returns:
         ChatResult:         Conversations chat history.
         ContextVariables:   Updated Context variables.
         "ConversableAgent":   Last speaker.
     """
+    if a_pause_event is None:
+        a_pause_event = asyncio.Event()
+        a_pause_event.set()
     # Let the pattern prepare the group chat and all its components
     # Only passing the necessary parameters that aren't already in the pattern
     (
@@ -192,6 +204,8 @@ async def a_initiate_group_chat(
     if last_agent is None:
         raise ValueError("No agent selected to start the conversation")
 
+    manager._a_pause_event = a_pause_event
+
     chat_result = await last_agent.a_initiate_chat(
         manager,
         message=last_message,  # type: ignore[arg-type]
@@ -218,6 +232,7 @@ def run_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    pause_event: threading.Event | None = None,
 ) -> RunResponseProtocol:
     """Run a group chat with multiple agents using the specified pattern.
 
@@ -239,6 +254,9 @@ def run_group_chat(
     Returns:
         RunResponseProtocol
     """
+    if pause_event is None:
+        pause_event = threading.Event()
+        pause_event.set()
     iostream = ThreadIOStream()
     all_agents = pattern.agents + ([pattern.user_agent] if pattern.user_agent else [])
     response = RunResponse(iostream, agents=all_agents)
@@ -252,6 +270,7 @@ def _initiate_group_chat(
         mask_llm_config: LLMConfig | None = mask_llm_config,
         iostream: ThreadIOStream = iostream,
         response: RunResponse = response,
+        pause_event: threading.Event = pause_event,
     ) -> None:
         with IOStream.set_default(iostream):
             try:
@@ -262,6 +281,7 @@ def _initiate_group_chat(
                     safeguard_policy=safeguard_policy,
                     safeguard_llm_config=safeguard_llm_config,
                     mask_llm_config=mask_llm_config,
+                    pause_event=pause_event,
                 )
 
                 IOStream.get_default().send(
@@ -280,6 +300,7 @@ def _initiate_group_chat(
         target=_initiate_group_chat,
     ).start()
 
+    response.pause_event = pause_event
     return response
 
 
@@ -291,6 +312,7 @@ async def a_run_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    a_pause_event: asyncio.Event | None = None,
 ) -> AsyncRunResponseProtocol:
     """Async version of run_group_chat for running group chats in async contexts.
 
@@ -312,6 +334,9 @@ async def a_run_group_chat(
     Returns:
         AsyncRunResponseProtocol
     """
+    if a_pause_event is None:
+        a_pause_event = asyncio.Event()
+        a_pause_event.set()
     iostream = AsyncThreadIOStream()
     all_agents = pattern.agents + ([pattern.user_agent] if pattern.user_agent else [])
     response = AsyncRunResponse(iostream, agents=all_agents)
@@ -325,6 +350,7 @@ async def _initiate_group_chat(
         mask_llm_config: LLMConfig | None = mask_llm_config,
         iostream: AsyncThreadIOStream = iostream,
         response: AsyncRunResponse = response,
+        a_pause_event: asyncio.Event = a_pause_event,
     ) -> None:
         with IOStream.set_default(iostream):
             try:
@@ -335,6 +361,7 @@ async def _initiate_group_chat(
                     safeguard_policy=safeguard_policy,
                     safeguard_llm_config=safeguard_llm_config,
                     mask_llm_config=mask_llm_config,
+                    a_pause_event=a_pause_event,
                 )
 
                 iostream.send(
@@ -364,6 +391,7 @@ def run_group_chat_iter(
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
     yield_on: Sequence[type["BaseEvent"]] | None = None,
+    pause_event: threading.Event | None = None,
 ) -> RunIterResponse:
     """Run a group chat with iterator-based stepped execution.
 
@@ -386,6 +414,9 @@ def run_group_chat_iter(
     Returns:
         RunIterResponse: An iterator that yields events as they occur.
     """
+    if pause_event is None:
+        pause_event = threading.Event()
+        pause_event.set()
     all_agents = pattern.agents + ([pattern.user_agent] if pattern.user_agent else [])
 
     def create_thread(iostream: ThreadIOStream) -> threading.Thread:
@@ -399,6 +430,7 @@ def _initiate_group_chat() -> None:
                         safeguard_policy=safeguard_policy,
                         safeguard_llm_config=safeguard_llm_config,
                         mask_llm_config=mask_llm_config,
+                        pause_event=pause_event,
                     )
 
                     IOStream.get_default().send(
@@ -415,11 +447,13 @@ def _initiate_group_chat() -> None:
 
         return threading.Thread(target=_initiate_group_chat, daemon=True)
 
-    return RunIterResponse(
+    response = RunIterResponse(
         start_thread_func=create_thread,
         yield_on=yield_on,
         agents=all_agents,
     )
+    response.pause_event = pause_event
+    return response
 
 
 @export_module("autogen.agentchat")
@@ -431,6 +465,7 @@ def a_run_group_chat_iter(
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
     yield_on: Sequence[type["BaseEvent"]] | None = None,
+    a_pause_event: asyncio.Event | None = None,
 ) -> AsyncRunIterResponse:
     """Async version of run_group_chat_iter for async contexts.
 
@@ -453,6 +488,9 @@ def a_run_group_chat_iter(
     Returns:
         AsyncRunIterResponse: An async iterator that yields events as they occur.
     """
+    if a_pause_event is None:
+        a_pause_event = asyncio.Event()
+        a_pause_event.set()
     all_agents = pattern.agents + ([pattern.user_agent] if pattern.user_agent else [])
 
     def create_thread(iostream: ThreadIOStream) -> threading.Thread:
@@ -464,6 +502,7 @@ async def _async_initiate_group_chat() -> None:
                 safeguard_policy=safeguard_policy,
                 safeguard_llm_config=safeguard_llm_config,
                 mask_llm_config=mask_llm_config,
+                a_pause_event=a_pause_event,
             )
 
             iostream.send(
@@ -485,8 +524,10 @@ def _run_in_thread() -> None:
 
         return threading.Thread(target=_run_in_thread, daemon=True)
 
-    return AsyncRunIterResponse(
+    reponse = AsyncRunIterResponse(
         start_thread_func=create_thread,
         yield_on=yield_on,
         agents=all_agents,
     )
+    response.a_pause_event = a_pause_event
+    return response
diff --git a/autogen/agentchat/groupchat.py b/autogen/agentchat/groupchat.py
index 538a2d456b..fd68920860 100644
--- a/autogen/agentchat/groupchat.py
+++ b/autogen/agentchat/groupchat.py
@@ -4,12 +4,15 @@
 #
 # Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
 # SPDX-License-Identifier: MIT
+import asyncio
 import copy
 import json
 import logging
 import random
 import re
 import sys
+import threading
+import time
 from collections.abc import Callable
 from dataclasses import dataclass, field
 from typing import Any, Literal
@@ -1091,6 +1094,8 @@ def __init__(
         human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "NEVER",
         system_message: str | list | None = "Group chat manager.",
         silent: bool = False,
+        pause_event: threading.Event | None = None,
+        a_pause_event: asyncio.Event | None = None,
         **kwargs: Any,
     ):
         if (
@@ -1116,6 +1121,8 @@ def __init__(
 
         self._last_speaker = None
         self._silent = silent
+        self._pause_event = pause_event
+        self._a_pause_event = a_pause_event
 
         # Order of register_reply is important.
         # Allow sync chat if initiated using initiate_chat
@@ -1228,6 +1235,10 @@ def run_chat(
                 a.previous_cache = a.client_cache
                 a.client_cache = self.client_cache
         for i in range(groupchat.max_round):
+            # Check pause before each round
+            if self._pause_event is not None:
+                while not self._pause_event.is_set():
+                    time.sleep(0.1)
             self._last_speaker = speaker
             groupchat.append(message, speaker)
             # broadcast the message to all agents except the speaker
@@ -1361,6 +1372,9 @@ async def a_run_chat(
                 a.previous_cache = a.client_cache
                 a.client_cache = self.client_cache
         for i in range(groupchat.max_round):
+            # Check pause before each round
+            if self._a_pause_event is not None:
+                await self._a_pause_event.wait()
             groupchat.append(message, speaker)
             self._last_speaker = speaker
 
