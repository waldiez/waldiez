diff --git a/autogen/agentchat/group/multi_agent_chat.py b/autogen/agentchat/group/multi_agent_chat.py
index 109063cfb7..dafcdafdfa 100644
--- a/autogen/agentchat/group/multi_agent_chat.py
+++ b/autogen/agentchat/group/multi_agent_chat.py
@@ -36,6 +36,7 @@ def initiate_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    pause_event: threading.Event | None = None,
 ) -> tuple[ChatResult, ContextVariables, "Agent"]:
     """Initialize and run a group chat using a pattern for configuration.
 
@@ -46,12 +47,17 @@ def initiate_group_chat(
         safeguard_policy: Optional safeguard policy dict or path to JSON file.
         safeguard_llm_config: Optional LLM configuration for safeguard checks.
         mask_llm_config: Optional LLM configuration for masking.
+        pause_event: Optional Event to pause the chat.
 
     Returns:
         ChatResult:         Conversations chat history.
         ContextVariables:   Updated Context variables.
         "ConversableAgent":   Last speaker.
     """
+    if pause_event is None:
+        pause_event = threading.Event()
+        pause_event.set()
+
     # Let the pattern prepare the group chat and all its components
     # Only passing the necessary parameters that aren't already in the pattern
     (
@@ -95,6 +101,8 @@ def initiate_group_chat(
     if last_agent is None:
         raise ValueError("No agent selected to start the conversation")
 
+    manager._pause_event = pause_event
+
     chat_result = last_agent.initiate_chat(
         manager,
         message=last_message,
@@ -115,6 +123,7 @@ async def a_initiate_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    a_pause_event: asyncio.Event | None = None,
 ) -> tuple[ChatResult, ContextVariables, "Agent"]:
     """Initialize and run a group chat using a pattern for configuration, asynchronously.
 
@@ -125,12 +134,16 @@ async def a_initiate_group_chat(
         safeguard_policy: Optional safeguard policy dict or path to JSON file.
         safeguard_llm_config: Optional LLM configuration for safeguard checks.
         mask_llm_config: Optional LLM configuration for masking.
+        a_pause_event: Optional Event to pause the chat.
 
     Returns:
         ChatResult:         Conversations chat history.
         ContextVariables:   Updated Context variables.
         "ConversableAgent":   Last speaker.
     """
+    if a_pause_event is None:
+        a_pause_event = asyncio.Event()
+        a_pause_event.set()
     # Let the pattern prepare the group chat and all its components
     # Only passing the necessary parameters that aren't already in the pattern
     (
@@ -174,6 +187,8 @@ async def a_initiate_group_chat(
     if last_agent is None:
         raise ValueError("No agent selected to start the conversation")
 
+    manager._a_pause_event = a_pause_event
+
     chat_result = await last_agent.a_initiate_chat(
         manager,
         message=last_message,  # type: ignore[arg-type]
@@ -194,11 +209,16 @@ def run_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    pause_event: threading.Event | None = None,
 ) -> RunResponseProtocol:
     iostream = ThreadIOStream()
     # todo: add agents
     response = RunResponse(iostream, agents=[])
 
+    if pause_event is None:
+        pause_event = threading.Event()
+        pause_event.set()
+
     def _initiate_group_chat(
         pattern: "Pattern" = pattern,
         messages: list[dict[str, Any]] | str = messages,
@@ -208,6 +228,7 @@ def _initiate_group_chat(
         mask_llm_config: LLMConfig | None = mask_llm_config,
         iostream: ThreadIOStream = iostream,
         response: RunResponse = response,
+        pause_event: threading.Event = pause_event,
     ) -> None:
         with IOStream.set_default(iostream):
             try:
@@ -218,6 +239,7 @@ def _initiate_group_chat(
                     safeguard_policy=safeguard_policy,
                     safeguard_llm_config=safeguard_llm_config,
                     mask_llm_config=mask_llm_config,
+                    pause_event=pause_event,
                 )
 
                 IOStream.get_default().send(
@@ -236,6 +258,7 @@ def _initiate_group_chat(
         target=_initiate_group_chat,
     ).start()
 
+    response.pause_event = pause_event
     return response
 
 
@@ -247,11 +270,16 @@ async def a_run_group_chat(
     safeguard_policy: dict[str, Any] | str | None = None,
     safeguard_llm_config: LLMConfig | None = None,
     mask_llm_config: LLMConfig | None = None,
+    a_pause_event: asyncio.Event | None = None,
 ) -> AsyncRunResponseProtocol:
     iostream = AsyncThreadIOStream()
     # todo: add agents
     response = AsyncRunResponse(iostream, agents=[])
 
+    if a_pause_event is None:
+        a_pause_event = asyncio.Event()
+        a_pause_event.set()
+
     async def _initiate_group_chat(
         pattern: "Pattern" = pattern,
         messages: list[dict[str, Any]] | str = messages,
@@ -261,6 +289,7 @@ async def _initiate_group_chat(
         mask_llm_config: LLMConfig | None = mask_llm_config,
         iostream: AsyncThreadIOStream = iostream,
         response: AsyncRunResponse = response,
+        a_pause_event: asyncio.Event = a_pause_event,
     ) -> None:
         with IOStream.set_default(iostream):
             try:
@@ -271,6 +300,7 @@ async def _initiate_group_chat(
                     safeguard_policy=safeguard_policy,
                     safeguard_llm_config=safeguard_llm_config,
                     mask_llm_config=mask_llm_config,
+                    a_pause_event=a_pause_event,
                 )
 
                 IOStream.get_default().send(
diff --git a/autogen/agentchat/groupchat.py b/autogen/agentchat/groupchat.py
index 707d529806..41003180c6 100644
--- a/autogen/agentchat/groupchat.py
+++ b/autogen/agentchat/groupchat.py
@@ -4,12 +4,15 @@
 #
 # Portions derived from  https://github.com/microsoft/autogen are under the MIT License.
 # SPDX-License-Identifier: MIT
+import asyncio
 import copy
 import json
 import logging
 import random
 import re
 import sys
+import threading
+import time
 from collections.abc import Callable
 from dataclasses import dataclass, field
 from typing import Any, Literal
@@ -1085,6 +1088,8 @@ def __init__(
         human_input_mode: Literal["ALWAYS", "NEVER", "TERMINATE"] = "NEVER",
         system_message: str | list | None = "Group chat manager.",
         silent: bool = False,
+        pause_event: threading.Event | None = None,
+        a_pause_event: asyncio.Event | None = None,
         **kwargs: Any,
     ):
         if (
@@ -1110,6 +1115,8 @@ def __init__(
 
         self._last_speaker = None
         self._silent = silent
+        self._pause_event = pause_event
+        self._a_pause_event = a_pause_event
 
         # Order of register_reply is important.
         # Allow sync chat if initiated using initiate_chat
@@ -1222,6 +1229,11 @@ def run_chat(
                 a.previous_cache = a.client_cache
                 a.client_cache = self.client_cache
         for i in range(groupchat.max_round):
+            # Check pause before each round
+            if self._pause_event is not None:
+                while not self._pause_event.is_set():
+                    time.sleep(0.1)
+
             self._last_speaker = speaker
             groupchat.append(message, speaker)
             # broadcast the message to all agents except the speaker
@@ -1355,6 +1367,10 @@ async def a_run_chat(
                 a.previous_cache = a.client_cache
                 a.client_cache = self.client_cache
         for i in range(groupchat.max_round):
+            # Check pause before each round
+            if self._a_pause_event is not None:
+                await self._a_pause_event.wait()
+
             groupchat.append(message, speaker)
             self._last_speaker = speaker
 
